% !Mode:: "TeX:UTF-8"

\chapter{研究背景}

	近年来，得益于算法的改进和算力的提升，神经网络在物体识别\cite{NIPS2012_4824}、目标检测\cite{girshick15fastrcnn}、语义分割\cite{Long_2015_CVPR}等计算机视觉的传统任务中取得了突破性进展。随后，神经网络在自然语言处理\cite{NIPS2014_5346}和强化学习\cite{44806}等领域也取得了不错的进展。工业界也已经将神经网络应用于自动驾驶、健康医疗、智慧城市等领域。
	
	相应的，神经网络模型的安全性以及带来的隐私问题也逐渐受到人们的关注。\citet{szegedy2014intriguing}首次发现在图片分类的任务上，通过对输入图片进行特定的微小修改，原本预测正确的深度神经网络就会输出完全不一样的标签。这些改动小到可能无法被人察觉\cite{carlini2017towards}，相应的被修改的样本就被称作对抗样本。更严重的是，由一个模型生成的对抗样本，被发现可以迁移到其他模型上，即让其他模型也产生错误的预测结果。\citet{sinha2018certifiable}甚至发现可以在物理世界中生成稳定的对抗样本，用于攻击Faster R-CNN\cite{NIPS2015_5638}和YOLO\cite{Redmon_2016_CVPR}这两个在工业界得到大规模应用的目标检测算法。如果自动驾驶汽车在对抗样本的影响下，无法识别出一个停车的指示牌，后果将不堪设想。
	
	因此，我们若想将神经网络模型应用于自动驾驶、恶意软件检测、物联网等安全性及其重要的领域，神经网络的安全性与稳定性是一个重要的研究课题。

\vspace{\baselineskip}
{\let\clearpage\relax \chapter{相关工作}}
\section{对抗样本攻击}
暂时仅考虑在图像分类上的对抗样本攻击算法。已经发现在目标检测、语义分割、自然语言处理以及强化学习等任务中都存在对抗样本的问题。

\citet{szegedy2014intriguing}将对抗样本的生成形式化为如下优化问题
\begin{align}
\min_{\bm{\rho}} c\lVert \bm{\rho} \rVert + \mathcal{L}(\bm{I}_c + \rho, \ell) \text{\quad s.t. } \bm{I}_c + \rho \in [0,1]^m,
\end{align}
其中$\bm{\rho}$为扰动，$\bm{I}_c$为原图像，$\ell$为攻击者希望网络所输出的结果，$\bm{\mathcal{L}}(\cdot , \cdot)$为衡量分类器好坏的损失函数。\citet{szegedy2014intriguing}使用Box-constrained L-BFGS算法求得扰动$\rho$的近似解，将其加到原图像上即得到对抗样本。此外，\citet{szegedy2014intriguing}还这些对抗样本的可迁移性，即由一个神经网络模型产生的对抗样本，能以不小的可能性来“骗过”另一个完成同样任务的网络模型，即使新模型的架构、训练算法、训练集与原模型完全不同。为了防御对抗样本，\citet{szegedy2014intriguing}也提出了对抗训练算法，即在训练的过程中生成对抗样本并把它加入训练集中用于训练，从而显式地提高模型的稳定性。

对抗训练过程中，对抗样本在不断生成。而Box-contrained L-BFGS算法为二阶算法，求解速度较慢。因此，\cite{szegedy2014intriguing}中的对抗训练效率较低。为了加速训练过程中对抗样本的生成，\citet{goodfellow2015explaining}提出了基于梯度的对抗样本生成算法，即
\begin{align}
\bm{\rho} = \epsilon \text{sign} (\nabla_{\bm{I}_c} \mathcal{L} (\bm{\theta}, \bm{I}_c, \ell)),
\end{align}
其中$\epsilon$为每个像素的最大扰动值，$\bm{\theta}$为神经网络的参数，$\nabla_{\bm{I}_c} \mathcal{L} (\bm{\theta}, \bm{I}_c, \ell)$为损失函数基于输入图像的梯度。利用反向传播算法，对抗样本生成的开销很小。使用符号函数是因为该算法是在$\ell_\infty$的范数下进行优化。\citet{goodfellow2015explaining}使用在损失函数附近的线性近似来得到对抗样本效果不错，提出了一个神经网络的局部可能没有我们所想象的那样非线性的假设。但这个假设未经进一步证实，并且受到了一些质疑\cite{tanay2016boundary}。

以上所谈到的都是单步算法，即只对输入图像进行一次扰动的算法。\citet{kurakin2016adversarial}提出了基于Projected Gradient Descent的Basic Iterative Method:
\begin{align}
\bm{I}_\rho^{i+1} = \text{Clip}_\epsilon \{\bm{I}_\rho^i + \alpha \text{sign} (\nabla_{\bm{I}_c} \mathcal{L} (\bm{\theta}, \bm{I}_c, \ell))\},
\end{align}
其中$\bm{I}_\rho^i$表示在经过$i$次迭代后的对抗样本。$\text{Clip}\{\cdot\}$将对抗样本限制在$\ell_\infty$范数下的$B_\epsilon(\bm{I}_c)$内，$\alpha$表示学习速率。

以上所谈到的算法都是计算针对$\ell_\infty$和$\ell_2$范数下的小扰动。
\citet{papernot2016limitations}考虑在$\ell_0$范数下的小扰动，即尽可能地扰动较少的像素点。\citet{papernot2016limitations}基本思路是，通过函数的输出相对于输入图像的Jacobian矩阵，启发式地找出两个像素点进行攻击。经过多次迭代，直至攻击成功，或者修改像素数达到上限，攻击失败。\citet{su2017pixel}利用进化算法发现可以在仅仅改变一个像素的情况下完成成功率极高的攻击，并且被“欺骗”的网络对自己错误的预测的置信度还很高。

\cite{papernot2016distillation}提出一种Distillation算法可以有效防御前面所谈到的\cite{szegedy2014intriguing}\cite{goodfellow2015explaining}\cite{papernot2016limitations}攻击。随后\citep{carlini2017towards}提出了三种算法，分别生成对应于$\ell_0,\ell_2,\ell_\infty$范数的对抗样本，从而将Distillation这一算法攻破。

上述方法都是针对每一张图片生成特定的扰动。\citet{Moosavi-Dezfooli_2017_CVPR}发现可以通过将这些扰动叠加，得到一个较为通用的微小扰动，从而能在许多输入图片上产生对抗样本。此外，\citet{Moosavi-Dezfooli_2017_CVPR}还发现这样的微小扰动甚至能迁移到不同网络结构的模型上。

\section{对抗样本防御}

针对对抗样本的防御措施可以简要分为两类。一类是在修改训练或者测试时的输入，另一类是修改网络结构或者重新训练。其中，第二类又可以分成异常检测（输入是否为对抗样本）和直接防御。

在第一类算法中，最直接的想法就是如\cite{szegedy2014intriguing}\cite{goodfellow2015explaining}中一样，在训练的过程中加入对抗样本。将对抗样本作为一种正则化的手段，能在一定程度上减轻过拟合\cite{miyato2018virtual}。但这样的算法不仅在生成对抗样本时带来极大的开销\cite{szegedy2014intriguing}，也隐式地扩大的数据集，从而使得训练成本大大增加。同时，\citet{Moosavi-Dezfooli_2017_CVPR}还发现对抗训练并不能消除对抗样本的存在。
此外，研究者也尝试使用JPEG压缩算法\cite{dziugaite2016study}、随机裁剪\cite{luo2015foveationbased}、随机重新调整大小并加以填充\cite{xie2017adversarial}来达到防御对抗样本的目的。

\citet{papernot2016distillation}将Distillation机制\cite{hinton2015distilling}利用在防御中，即使用原网络的输出作为新网络（同原网络结构、算法等完全相同），认为这样能够一定程度上缓解过拟合现象，并从实验上验证新网络能有效抵抗对抗样本。不过该算法已经被\cite{carlini2017towards}攻破。

类似的，\citet{CisseBGDU17}通过在损失函数中加入权重矩阵的最大奇异值的惩罚项，来有效降低神经网络损失函数的Lipschitz常数，从而使得损失函数更加平滑，对抗样本更不容易出现。 	

\vspace{\baselineskip}
{\let\clearpage\relax \chapter{研究内容与解决方案}}

\section{研究目标}

结合小样本学习、迁移学习、元学习方面的工作，改进现有的黑盒攻击算法，提高对抗样本的可迁移性。具体来说，就是在保证较高成功率的情况下，生成更加难以察觉的对抗样本，或者减少问询次数、降低所需计算资源。

\section{研究内容}

本项目主要由以下几个任务组成：
\begin{enumerate}
\item 对黑盒攻击这一子领域进行文献搜索与阅读，了解最新的研究成果，并且完成一个对这个子领域的简单综述报告。
\item 对小样本学习、迁移学习和元学习领域进行适当涉猎，了解比较有代表性的工作，思考是否有可能借鉴其中的思想，来改善对抗样本的可迁移性。尝试提出一些对现有算法的改进。
\item 深入了解tensorflow框架和cleverhans函数库，复现黑盒攻击领域最为先进的算法，通过亲自实验来了解其利弊（扰动是否过大、是否需要过多问询次数、是否训练时间过长等等），从而思考可以改进的地方。
\item 将2、3两步所得到的初步结论综合，提出一些可能的改进算法。然后实现这些算法，根据实验结果逐步修正自己的想法，并提出最终的模型。
\item 实现最终模型，并进行细致的分析，希望从中能得到一些关于神经网络、对抗样本或者黑盒攻击的新见解。
\end{enumerate}

\section{技术方案}

在Google Cloud上搭建VM Instance，利用Google Cloud提供的计算资源进行实验。熟练使用Google官方提供的Cleverhans函数库，以及原作者开源的代码库进行实验。

拟使用MNIST\cite{lecun1998mnist}、CIFAR-10\cite{cifar10}、SVHN\cite{netzer2011reading}等小规模数据集进行初步实验。

\vspace{\baselineskip}
{\let\clearpage\relax \chapter{研究难点}}

神经网络仍缺乏较好的数学支撑。研究者们现在暂时只能在一些非常简单的模型上（如深层线性网络）进行一些理论建模和证明。而对实际使用的高度非线性网络（如ResNet）暂时没有任何方法分析。就连网络中的某一个成分，例如Batch Normalization，暂时都没有很好的解释与分析。

因此，现阶段的工作暂时局限于根据直觉进行实验，从而得到一些结论，再提出猜想和假设并进行验证的流程。这一过程中，得到的结论是否正确，能否根据假设和猜想设计合理的实验来验证，需要深刻的思考与试错。

就这一领域而言，对抗样本为什么会存在仍然是个未解之谜。\citet{goodfellow2015explaining}提出了神经网络局部线性的假设，但这一假设未经进一步证实，并且与不少学者的实验结果相悖。此外，对抗样本的可迁移性也是另一大难题，现在几乎没有任何工作能较好的解释这一现象。

\vspace{\baselineskip}
{\let\clearpage\relax \chapter{预期结果}}

希望能够得到一个在保证较高攻击成功率下，大幅减少对目标模型问询次数、减小所需扰动的黑盒攻击算法。如果能得到这样一个算法，则进一步进行分析，找出算法中真正起作用的成分，希望能以此加深对神经网络这一强力工具的理解。


\vspace{\baselineskip}
{\let\clearpage\relax \chapter{进度安排}}
\begin{center}
	\begin{tabular}{| m{0.35\textwidth} | m{0.6\textwidth}|}
	\hline
	2018年10月 - 2018年11月 & 进行开题准备，了解对抗样本这一领域内比较有代表性的工作，对整个领域有一定了解 \\
	\hline
	2018年11月 - 2018年12月 & 对黑盒攻击与防御这一子领域进行深入了解 \\
	\hline
	2018年12月 - 2019年 1月 & 对小样本学习、迁移学习、元学习这一领域进行深入了解 \\
	\hline
	2019年 1月 - 2019年 2月 & 复现领域内最新的成果，从而深刻理解各种算法的利弊，并提出一系列改进措施 \\
	\hline
	2019年 2月 - 2019年 3月 & 实现自己的改进方案，并逐步修正自己的想法 \\
	\hline 
	2019年 3月 - 2019年 4月 & 整理已有的实验结果并进行分析，完成中期答辩 \\
	\hline 
	2019年 4月 - 2019年 5月 & 完善最终算法和模型，完成毕业设计初稿 \\
	\hline
	2019年 5月 - 2019年 6月 & 完成毕业设计终稿以及毕设答辩。 \\
	\hline
	\end{tabular}
\end{center}